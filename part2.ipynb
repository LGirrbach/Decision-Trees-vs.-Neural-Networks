{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "disabled-public",
   "metadata": {},
   "source": [
    "# Oldies but Goldies Project: Decision Trees and Neural Networks\n",
    "\n",
    "<p style=\"text-align: center;\">Leander Girrbach <br> <a href=\"mailto:girrbach@cl.uni-heidelberg.de\">girrbach@cl.uni-heidelberg.de</a></p>\n",
    "\n",
    "**Important note**: This project consists of 2 parts:\n",
    " 1. Differentiable decision trees\n",
    " 2. Distilling neural networks into (conventional) decision trees\n",
    "\n",
    "Both parts explore different ideas for combining neural networks (mostly MLPs) and decision trees (or random forests).\n",
    "\n",
    "## Part 2: Distilling Neural Networks into Decision Trees\n",
    "\n",
    "### Idea\n",
    "\n",
    "When training a neural network on a classification task, we receive an uninterpretable classifier. A subfield of deep learning is distillation, which seeks to make a smaller neural network behave like a larger neural network. One of the main reasons for distillation is to reduce the computational power needed to solve a certain problem.\n",
    "\n",
    "The same idea can be used to a learn a completely different type of classifier to imitate the calculations of the neural network. To this end, we view the neural network as a multivariate function mapping input vectors to probability distributions over the labels $\\mathbb{R}^{d_\\text{in}} \\rightarrow  \\mathbb{R}^{\\#\\text{labels}}$.\n",
    "\n",
    "We can use a trained teacher model and some data to train another student classifier to behave like the teacher neural network. We can either require the student classifier to only output the same labels as the teacher neural network or also require the student classifier to output the same probability distribution over labels as the teacher neural network.\n",
    "\n",
    "### Method\n",
    "\n",
    "Given a dataset of paired inputs and labels $(\\mathcal{X}_{\\text{train}}, \\mathcal{Y}_{\\text{train}})$, I train a neural networks on $\\mathcal{X}_{\\text{train}}$ to predict the corresponding labels. This model serves as the teacher. After the training process, I use another set of inputs $\\mathcal{X}_{\\text{distill}}$ to calculate the label distribution induced by the teacher model. By taking the $\\arg\\max$, I can get the predicted labels.\n",
    "\n",
    "For $\\mathcal{X}_{\\text{distill}}$, I evaluate $2$ options:\n",
    "\n",
    " * The train dataset $\\mathcal{X}_{\\text{train}}$\n",
    " * A larger dataset containing documents from the same domain\n",
    "\n",
    "Using this information, I train Decision Tree classifiers / Random Forest classifiers to predict the same labels on $\\mathcal{X}_{\\text{distill}}$ as the teachter model. Furthermore, I train Decision Tree regressors / Random Forest regressors to predict the same probability distributions on $\\mathcal{X}_{\\text{distill}}$ as the teacher model.\n",
    "\n",
    "### Data\n",
    "\n",
    "For $\\mathcal{X}_{\\text{train}}$, I reuse the 20 newsgroups dataset as in Part 1. For $\\mathcal{X}_{\\text{distill}}$, I add documents from the AG NEWS-dataset as provided by `torchtext`. Preprocessing is the same as in Part 1, namely lowercasing, tokenising, lemmatising, and filtering stopwords.\n",
    "\n",
    "### Models\n",
    "\n",
    "I evaluate the distilling neural networks on $2$ types of neural networks (trained on the same data):\n",
    "\n",
    " 1. A MLP feedforward neural network. Here, I represent documents by SVD truncated tf-idf weighted bag-of-words features.\n",
    " 2. A Bidirectional LSTM classifier. Here, tokens are represented by pretrained word2vec embeddings (provided by `gensim`)\n",
    "\n",
    "The MLP has $2$ hidden layers with $128$ units each. The LSTM has also has $2$ layers with $128$ units each (both directions). Both models are trained using the Adam optimiser with default parameters (as specified by `sklearn`/`keras`) by minimising the cross-entropy of the predicted label probabilities and the real one-hot-encoded labels. Batch size is $32$ in both cases. The LSTM is trained for $20$ epochs.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "I report the following metrics:\n",
    "\n",
    " * Test set accuracy (on the 20 newsgroups test set)\n",
    " * Reference accuracy (on the 20 newsgroups test set): Here, the predictions of the teacher model are treated as true labels\n",
    " * Train set accuracy (on the 20 newsgroups train set)\n",
    " * R2 coefficient of determination between probabilities predicted by student and teacher models\n",
    " * KL-Divergence between probabilities predicted by student and teacher models. For decision trees, this makes only sense for the regressors, because classification trees return only one label (one-hot distribution)\n",
    " \n",
    "With these metrics, both a good impression of the overall performance of the models (wrt. the data) and the approximation performance (wrt. approximating the teacher model) can be evaluated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lonely-planning",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader as gensim\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.notebook import trange\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "neither-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeds for determinism\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Who cares for warnings?\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ethical-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the 20 newsgroups dataset as our main dataset\n",
    "train_data = fetch_20newsgroups(subset='train')\n",
    "test_data = fetch_20newsgroups(subset='test')\n",
    "X_train_raw, y_train = train_data.data, train_data.target\n",
    "X_test_raw, y_test = test_data.data, test_data.target\n",
    "\n",
    "# Then we use the AG News dataset for synthetic data\n",
    "# Here, we only need the documents, not the labels\n",
    "agnews_train, agnews_test = AG_NEWS()\n",
    "_, agnews_X_train = zip(*agnews_train)\n",
    "_, agnews_X_test = zip(*agnews_test)\n",
    "\n",
    "# For synthetic data, we use the train portion of the 20 newsgroups dataset\n",
    "# and the AG News data\n",
    "X_synthetic_raw = np.concatenate([X_train_raw, agnews_X_train, agnews_X_test])\n",
    "# For synthetic data, we do not have labels yet. First, we need to train a classifier, which we will do later\n",
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "excellent-constraint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed1862ec0894b48946b1f022b84a212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75374d218b7b475cbd30a00000e08122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7532 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ca72a86d0648eba9d3d6a126a228df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy_preprocessor = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def preprocess(documents):\n",
    "    processed_documents = []\n",
    "    documents = tqdm(documents)\n",
    "    documents.set_description(\"Processing documents\")\n",
    "    for document in documents:\n",
    "        document = document.lower()\n",
    "        document = spacy_preprocessor(document)\n",
    "        document = [token.lemma_ for token in document if not token.is_stop]\n",
    "        processed_documents.append(\" \".join(document))\n",
    "    return processed_documents\n",
    "\n",
    "X_train_text = preprocess(X_train_raw)\n",
    "X_test_text = preprocess(X_test_raw)\n",
    "X_synthetic_text = preprocess(X_synthetic_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "moral-geneva",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding labels by integers\n",
      "Constructing tf-idf weighted document-term matrix\n",
      "Performing Matrix factorisation using SVD\n",
      "Making sequence data\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoding labels by integers\")\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "print(\"Constructing tf-idf weighted document-term matrix\")\n",
    "vectoriser = TfidfVectorizer(tokenizer=str.split, min_df=10)\n",
    "vectoriser.fit(X_train_text)\n",
    "X_train_vector = vectoriser.transform(X_train_text)\n",
    "X_test_vector = vectoriser.transform(X_test_text)\n",
    "X_synthetic_vector = vectoriser.transform(X_synthetic_text)\n",
    "\n",
    "print(\"Performing Matrix factorisation using SVD\")\n",
    "svd = TruncatedSVD(n_components=512)\n",
    "svd.fit(X_train_vector)\n",
    "X_train_vector = svd.transform(X_train_vector)\n",
    "X_test_vector = svd.transform(X_test_vector)\n",
    "X_synthetic_vector = svd.transform(X_synthetic_vector)\n",
    "\n",
    "print(\"Making sequence data\")\n",
    "# From https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "X_train_sequence = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test_sequence = tokenizer.texts_to_sequences(X_test_text)\n",
    "X_synthetic_sequence = tokenizer.texts_to_sequences(X_synthetic_text)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 400\n",
    "\n",
    "X_train_sequence = pad_sequences(X_train_sequence, padding='post', maxlen=maxlen)\n",
    "X_test_sequence = pad_sequences(X_test_sequence, padding='post', maxlen=maxlen)\n",
    "X_synthetic_sequence = pad_sequences(X_synthetic_sequence, padding='post', maxlen=maxlen)\n",
    "y_train_sequence = np_utils.to_categorical(y_train)\n",
    "y_test_sequence = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "satisfied-reaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the LSTM model\n",
      "Building the embedding matrix\n",
      "Building Keras model\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 300)         35796900  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 256)         439296    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                5140      \n",
      "=================================================================\n",
      "Total params: 36,635,576\n",
      "Trainable params: 838,676\n",
      "Non-trainable params: 35,796,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Building the LSTM model\")\n",
    "print(\"Building the embedding matrix\")\n",
    "embeddings = gensim.load('word2vec-google-news-300')\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = embeddings[word]\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "# From https://keras.io/examples/nlp/bidirectional_lstm_imdb/\n",
    "print(\"Building Keras model\")\n",
    "# Input for variable-length sequences of integers\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 50-dimensional vector using pretrained embeddings\n",
    "x = layers.Embedding(vocab_size, 300, weights=[embedding_matrix], trainable=False)(inputs)\n",
    "# Add 2 bidirectional LSTMs\n",
    "x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(128))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(20, activation=\"softmax\")(x)\n",
    "reference_lstm = keras.Model(inputs, outputs)\n",
    "reference_lstm.summary()\n",
    "\n",
    "reference_lstm.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "collectible-sussex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "354/354 [==============================] - 225s 620ms/step - loss: 2.4728 - accuracy: 0.1927\n",
      "Epoch 2/20\n",
      "354/354 [==============================] - 216s 609ms/step - loss: 1.7650 - accuracy: 0.4015\n",
      "Epoch 3/20\n",
      "354/354 [==============================] - 217s 613ms/step - loss: 1.3932 - accuracy: 0.5218\n",
      "Epoch 4/20\n",
      "354/354 [==============================] - 218s 616ms/step - loss: 1.0735 - accuracy: 0.6187\n",
      "Epoch 5/20\n",
      "354/354 [==============================] - 215s 607ms/step - loss: 0.9388 - accuracy: 0.6752\n",
      "Epoch 6/20\n",
      "354/354 [==============================] - 218s 616ms/step - loss: 0.7391 - accuracy: 0.7541\n",
      "Epoch 7/20\n",
      "354/354 [==============================] - 229s 646ms/step - loss: 0.6485 - accuracy: 0.7915\n",
      "Epoch 8/20\n",
      "354/354 [==============================] - 215s 607ms/step - loss: 0.5232 - accuracy: 0.8240\n",
      "Epoch 9/20\n",
      "354/354 [==============================] - 215s 607ms/step - loss: 0.4528 - accuracy: 0.8531\n",
      "Epoch 10/20\n",
      "354/354 [==============================] - 215s 609ms/step - loss: 0.3659 - accuracy: 0.8842\n",
      "Epoch 11/20\n",
      "354/354 [==============================] - 214s 606ms/step - loss: 0.3052 - accuracy: 0.9013\n",
      "Epoch 12/20\n",
      "354/354 [==============================] - 215s 609ms/step - loss: 0.2513 - accuracy: 0.9177\n",
      "Epoch 13/20\n",
      "354/354 [==============================] - 264s 746ms/step - loss: 0.1905 - accuracy: 0.9378\n",
      "Epoch 14/20\n",
      "354/354 [==============================] - 302s 854ms/step - loss: 0.1574 - accuracy: 0.9510\n",
      "Epoch 15/20\n",
      "354/354 [==============================] - 309s 874ms/step - loss: 0.1508 - accuracy: 0.9504\n",
      "Epoch 16/20\n",
      "354/354 [==============================] - 318s 899ms/step - loss: 0.1220 - accuracy: 0.9616\n",
      "Epoch 17/20\n",
      "354/354 [==============================] - 317s 896ms/step - loss: 0.0965 - accuracy: 0.9721\n",
      "Epoch 18/20\n",
      "354/354 [==============================] - 318s 899ms/step - loss: 0.0825 - accuracy: 0.9721\n",
      "Epoch 19/20\n",
      "354/354 [==============================] - 255s 719ms/step - loss: 0.0601 - accuracy: 0.9808\n",
      "Epoch 20/20\n",
      "354/354 [==============================] - 218s 615ms/step - loss: 0.0604 - accuracy: 0.9838\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Train Reference MLP: 2-layer MLP on train portion\n",
    "reference_mlp = MLPClassifier((128, 128,), batch_size=32)\n",
    "reference_mlp.fit(X_train_vector, y_train)\n",
    "\n",
    "# Train Reference Decision Tree\n",
    "reference_decision_tree = DecisionTreeClassifier()\n",
    "reference_decision_tree.fit(X_train_vector, y_train)\n",
    "\n",
    "# Train reference Random Forest\n",
    "reference_random_forest = RandomForestClassifier(n_estimators=512, n_jobs=20)\n",
    "reference_random_forest.fit(X_train_vector, y_train)\n",
    "\n",
    "# Train reference BiLSTM model\n",
    "reference_lstm.fit(X_train_sequence, y_train_sequence, batch_size=32, epochs=20)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "quarterly-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic labels from MLP classifier\n",
    "# First, get labels for the whole synthetic data\n",
    "y_synthetic_labels = reference_mlp.predict(X_synthetic_vector)\n",
    "y_synthetic_probabilities = reference_mlp.predict_proba(X_synthetic_vector)\n",
    "# Also get probabilities for only the 20 newsgroups train set\n",
    "y_train_probabilities = reference_mlp.predict_proba(X_train_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "processed-operation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training synthetic tree classifier\n",
      "Training synthetic tree regressor\n",
      "Training synthetic forest classifier\n",
      "Training synthetic forest regressor\n",
      "Training train tree regressor\n",
      "Training train forest regressor\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Now we distill the MLP into different Decision Tree/Random Forest models\n",
    "# using the predictions\n",
    "synthetic_decision_tree_classifier = DecisionTreeClassifier()\n",
    "synthetic_decision_tree_regressor = DecisionTreeRegressor()\n",
    "synthetic_random_forest_classifier = RandomForestClassifier(n_estimators=512, n_jobs=-1)\n",
    "synthetic_random_forest_regressor = RandomForestRegressor(n_estimators=512, n_jobs=-1)\n",
    "\n",
    "# Since the MLP can fit the train data almost perfectly, we don't need to train\n",
    "# a decision tree on the predicted train labels\n",
    "train_decision_tree_regressor = DecisionTreeRegressor()\n",
    "train_random_forest_regressor = RandomForestRegressor(n_estimators=512, n_jobs=-1)\n",
    "\n",
    "print(\"Training synthetic tree classifier\")\n",
    "synthetic_decision_tree_classifier.fit(X_synthetic_vector, y_synthetic_labels)\n",
    "print(\"Training synthetic tree regressor\")\n",
    "synthetic_decision_tree_regressor.fit(X_synthetic_vector, y_synthetic_probabilities)\n",
    "print(\"Training synthetic forest classifier\")\n",
    "synthetic_random_forest_classifier.fit(X_synthetic_vector, y_synthetic_labels)\n",
    "print(\"Training synthetic forest regressor\")\n",
    "synthetic_random_forest_regressor.fit(X_synthetic_vector, y_synthetic_probabilities)\n",
    "print(\"Training train tree regressor\")\n",
    "train_decision_tree_regressor.fit(X_train_vector, y_train_probabilities)\n",
    "print(\"Training train forest regressor\")\n",
    "train_random_forest_regressor.fit(X_train_vector, y_train_probabilities)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "surprised-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ref = reference_mlp.predict(X_test_vector)\n",
    "y_ref_probabilities = reference_mlp.predict_proba(X_test_vector)\n",
    "\n",
    "def get_metrics(y_pred_test, y_pred_train, y_pred_test_probabilities):\n",
    "    return [\n",
    "        accuracy_score(y_test, y_pred_test),\n",
    "        accuracy_score(y_ref, y_pred_test),\n",
    "        accuracy_score(y_train, y_pred_train),\n",
    "        r2_score(y_ref_probabilities, y_pred_test_probabilities),\n",
    "        np.mean(entropy(y_ref_probabilities, qk=y_pred_test_probabilities, axis=1))\n",
    "    ]\n",
    "\n",
    "def evaluate_classifier(classifier):\n",
    "    y_pred_test = classifier.predict(X_test_vector)\n",
    "    y_pred_train = classifier.predict(X_train_vector)\n",
    "    y_pred_test_probabilities = classifier.predict_proba(X_test_vector)\n",
    "    \n",
    "    return get_metrics(y_pred_test, y_pred_train, y_pred_test_probabilities)\n",
    "\n",
    "    \n",
    "def evaluate_regressor(regressor):\n",
    "    y_pred_test_probabilities = regressor.predict(X_test_vector)\n",
    "    y_pred_test = np.argmax(y_pred_test_probabilities, axis=1)\n",
    "    y_pred_train = np.argmax(regressor.predict(X_train_vector), axis=1)\n",
    "    \n",
    "    return get_metrics(y_pred_test, y_pred_train, y_pred_test_probabilities)\n",
    "\n",
    "def evaluate_lstm(lstm):\n",
    "    y_pred_test_probabilities = lstm.predict(X_test_sequence)\n",
    "    y_pred_test = np.argmax(y_pred_test_probabilities, axis=1)\n",
    "    y_pred_train = np.argmax(lstm.predict(X_train_sequence), axis=1)\n",
    "    \n",
    "    return get_metrics(y_pred_test, y_pred_train, y_pred_test_probabilities)\n",
    "\n",
    "    \n",
    "results = [\n",
    "    [\"Reference MLP\"] + evaluate_classifier(reference_mlp),\n",
    "    [\"Reference LSTM\"] + evaluate_lstm(reference_lstm),\n",
    "    [\"Reference Decision Tree\"] + evaluate_classifier(reference_decision_tree),\n",
    "    [\"Reference Random Forest\"] + evaluate_classifier(reference_random_forest),\n",
    "    [\"(Train only) Distilled Decision Tree Regressor\"] + evaluate_regressor(train_decision_tree_regressor),\n",
    "    [\"(Train only) Distilled Random Forest Regressor\"] + evaluate_regressor(train_random_forest_regressor),\n",
    "    [\"(Synthetic) Distilled Decision Tree Classifier\"] + evaluate_classifier(synthetic_decision_tree_classifier),\n",
    "    [\"(Synthetic) Distilled Random Forest Classifier\"] + evaluate_classifier(synthetic_random_forest_classifier),\n",
    "    [\"(Synthetic) Distilled Decision Tree Regressor\"] + evaluate_regressor(synthetic_decision_tree_regressor),\n",
    "    [\"(Synthetic) Distilled Random Forest Regressor\"] + evaluate_regressor(synthetic_random_forest_regressor),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "worthy-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['Model', \"Test Accuracy\", \"Reference Accuracy\", \"Train Accuracy\",\n",
    "           \"R2\", \"Test KL-Divergence\"]\n",
    "result_dataframe_mlp = pd.DataFrame(results, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "appointed-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic labels from LSTM classifier\n",
    "# First, get labels for the whole synthetic data\n",
    "y_synthetic_probabilities = reference_lstm.predict(X_synthetic_sequence)\n",
    "y_synthetic_labels = y_synthetic_probabilities.argmax(axis=1)\n",
    "# Also get probabilities for only the 20 newsgroups train set\n",
    "y_train_probabilities = reference_lstm.predict(X_train_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "simple-bikini",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training synthetic tree classifier\n",
      "Training synthetic tree regressor\n",
      "Training synthetic forest classifier\n",
      "Training synthetic forest regressor\n",
      "Training train tree regressor\n",
      "Training train forest regressor\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Now we distill the LSTM into different Decision Tree/Random Forest models\n",
    "# using the predictions\n",
    "synthetic_decision_tree_classifier = DecisionTreeClassifier()\n",
    "synthetic_decision_tree_regressor = DecisionTreeRegressor()\n",
    "synthetic_random_forest_classifier = RandomForestClassifier(n_estimators=512, n_jobs=-1)\n",
    "synthetic_random_forest_regressor = RandomForestRegressor(n_estimators=512, n_jobs=-1)\n",
    "\n",
    "# Since the MLP can fit the train data almost perfectly, we don't need to train\n",
    "# a decision tree on the predicted train labels\n",
    "train_decision_tree_regressor = DecisionTreeRegressor()\n",
    "train_random_forest_regressor = RandomForestRegressor(n_estimators=512, n_jobs=-1)\n",
    "\n",
    "print(\"Training synthetic tree classifier\")\n",
    "synthetic_decision_tree_classifier.fit(X_synthetic_vector, y_synthetic_labels)\n",
    "print(\"Training synthetic tree regressor\")\n",
    "synthetic_decision_tree_regressor.fit(X_synthetic_vector, y_synthetic_probabilities)\n",
    "print(\"Training synthetic forest classifier\")\n",
    "synthetic_random_forest_classifier.fit(X_synthetic_vector, y_synthetic_labels)\n",
    "print(\"Training synthetic forest regressor\")\n",
    "synthetic_random_forest_regressor.fit(X_synthetic_vector, y_synthetic_probabilities)\n",
    "print(\"Training train tree regressor\")\n",
    "train_decision_tree_regressor.fit(X_train_vector, y_train_probabilities)\n",
    "print(\"Training train forest regressor\")\n",
    "train_random_forest_regressor.fit(X_train_vector, y_train_probabilities)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "younger-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ref_probabilities = reference_lstm.predict(X_test_sequence)\n",
    "y_ref = y_ref_probabilities.argmax(axis=1)\n",
    "\n",
    "def get_metrics(y_pred_test, y_pred_train, y_pred_test_probabilities):\n",
    "    return [\n",
    "        accuracy_score(y_test, y_pred_test),\n",
    "        accuracy_score(y_ref, y_pred_test),\n",
    "        accuracy_score(y_train, y_pred_train),\n",
    "        r2_score(y_ref_probabilities, y_pred_test_probabilities),\n",
    "        np.mean(entropy(y_ref_probabilities, qk=y_pred_test_probabilities, axis=1))\n",
    "    ]\n",
    "\n",
    "def evaluate_classifier(classifier):\n",
    "    y_pred_test = classifier.predict(X_test_vector)\n",
    "    y_pred_train = classifier.predict(X_train_vector)\n",
    "    y_pred_test_probabilities = classifier.predict_proba(X_test_vector)\n",
    "    \n",
    "    return get_metrics(y_pred_test, y_pred_train, y_pred_test_probabilities)\n",
    "    \n",
    "def evaluate_regressor(regressor):\n",
    "    y_pred_test_probabilities = regressor.predict(X_test_vector)\n",
    "    y_pred_test = np.argmax(y_pred_test_probabilities, axis=1)\n",
    "    y_pred_train = np.argmax(regressor.predict(X_train_vector), axis=1)\n",
    "    \n",
    "    return get_metrics(y_pred_test, y_pred_train, y_pred_test_probabilities)\n",
    "\n",
    "def evaluate_lstm(lstm):\n",
    "    y_pred_test_probabilities = lstm.predict(X_test_sequence)\n",
    "    y_pred_test = np.argmax(y_pred_test_probabilities, axis=1)\n",
    "    y_pred_train = np.argmax(lstm.predict(X_train_sequence), axis=1)\n",
    "    \n",
    "    return get_metrics(y_pred_test, y_pred_train, y_pred_test_probabilities)\n",
    "\n",
    "results = [\n",
    "    [\"Reference LSTM\"] + evaluate_lstm(reference_lstm),\n",
    "    [\"Reference MLP\"] + evaluate_classifier(reference_mlp),\n",
    "    [\"Reference Decision Tree\"] + evaluate_classifier(reference_decision_tree),\n",
    "    [\"Reference Random Forest\"] + evaluate_classifier(reference_random_forest),\n",
    "    [\"(Train only) Distilled Decision Tree Regressor\"] + evaluate_regressor(train_decision_tree_regressor),\n",
    "    [\"(Train only) Distilled Random Forest Regressor\"] + evaluate_regressor(train_random_forest_regressor),\n",
    "    [\"(Synthetic) Distilled Decision Tree Classifier\"] + evaluate_classifier(synthetic_decision_tree_classifier),\n",
    "    [\"(Synthetic) Distilled Random Forest Classifier\"] + evaluate_classifier(synthetic_random_forest_classifier),\n",
    "    [\"(Synthetic) Distilled Decision Tree Regressor\"] + evaluate_regressor(synthetic_decision_tree_regressor),\n",
    "    [\"(Synthetic) Distilled Random Forest Regressor\"] + evaluate_regressor(synthetic_random_forest_regressor),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "pleased-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['Model', \"Test Accuracy\", \"Reference Accuracy\", \"Train Accuracy\",\n",
    "           \"R2\", \"Test KL-Divergence\"]\n",
    "result_dataframe_lstm = pd.DataFrame(results, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "respected-promotion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Reference Accuracy</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Absolute Error</th>\n",
       "      <th>Test KL-Divergence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reference MLP</td>\n",
       "      <td>0.752257</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reference LSTM</td>\n",
       "      <td>0.773898</td>\n",
       "      <td>0.707780</td>\n",
       "      <td>0.987538</td>\n",
       "      <td>0.500799</td>\n",
       "      <td>1.661332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reference Decision Tree</td>\n",
       "      <td>0.423526</td>\n",
       "      <td>0.436139</td>\n",
       "      <td>0.999912</td>\n",
       "      <td>-0.227965</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reference Random Forest</td>\n",
       "      <td>0.714153</td>\n",
       "      <td>0.725438</td>\n",
       "      <td>0.999912</td>\n",
       "      <td>0.379902</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Train only) Distilled Decision Tree Regressor</td>\n",
       "      <td>0.430563</td>\n",
       "      <td>0.439989</td>\n",
       "      <td>0.997348</td>\n",
       "      <td>-0.198031</td>\n",
       "      <td>15.219741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Train only) Distilled Random Forest Regressor</td>\n",
       "      <td>0.658391</td>\n",
       "      <td>0.667552</td>\n",
       "      <td>0.996995</td>\n",
       "      <td>0.487675</td>\n",
       "      <td>1.223216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Synthetic) Distilled Decision Tree Classifier</td>\n",
       "      <td>0.343601</td>\n",
       "      <td>0.350903</td>\n",
       "      <td>0.997348</td>\n",
       "      <td>-0.417319</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Synthetic) Distilled Random Forest Classifier</td>\n",
       "      <td>0.619756</td>\n",
       "      <td>0.638210</td>\n",
       "      <td>0.997348</td>\n",
       "      <td>0.274613</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Synthetic) Distilled Decision Tree Regressor</td>\n",
       "      <td>0.346256</td>\n",
       "      <td>0.356745</td>\n",
       "      <td>0.997348</td>\n",
       "      <td>-0.273317</td>\n",
       "      <td>11.981914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Synthetic) Distilled Random Forest Regressor</td>\n",
       "      <td>0.579395</td>\n",
       "      <td>0.593733</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.385302</td>\n",
       "      <td>1.498645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Model  Test Accuracy  \\\n",
       "0                                   Reference MLP       0.752257   \n",
       "1                                  Reference LSTM       0.773898   \n",
       "2                         Reference Decision Tree       0.423526   \n",
       "3                         Reference Random Forest       0.714153   \n",
       "4  (Train only) Distilled Decision Tree Regressor       0.430563   \n",
       "5  (Train only) Distilled Random Forest Regressor       0.658391   \n",
       "6  (Synthetic) Distilled Decision Tree Classifier       0.343601   \n",
       "7  (Synthetic) Distilled Random Forest Classifier       0.619756   \n",
       "8   (Synthetic) Distilled Decision Tree Regressor       0.346256   \n",
       "9   (Synthetic) Distilled Random Forest Regressor       0.579395   \n",
       "\n",
       "   Reference Accuracy  Train Accuracy  Absolute Error  Test KL-Divergence  \n",
       "0            1.000000        0.997348        1.000000            0.000000  \n",
       "1            0.707780        0.987538        0.500799            1.661332  \n",
       "2            0.436139        0.999912       -0.227965                 inf  \n",
       "3            0.725438        0.999912        0.379902                 inf  \n",
       "4            0.439989        0.997348       -0.198031           15.219741  \n",
       "5            0.667552        0.996995        0.487675            1.223216  \n",
       "6            0.350903        0.997348       -0.417319                 inf  \n",
       "7            0.638210        0.997348        0.274613                 inf  \n",
       "8            0.356745        0.997348       -0.273317           11.981914  \n",
       "9            0.593733        0.997260        0.385302            1.498645  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dataframe_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "seventh-decrease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Reference Accuracy</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>R2</th>\n",
       "      <th>Test KL-Divergence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reference LSTM</td>\n",
       "      <td>0.773898</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reference MLP</td>\n",
       "      <td>0.752257</td>\n",
       "      <td>0.707780</td>\n",
       "      <td>0.997348</td>\n",
       "      <td>0.459579</td>\n",
       "      <td>3.902797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reference Decision Tree</td>\n",
       "      <td>0.423526</td>\n",
       "      <td>0.411710</td>\n",
       "      <td>0.999912</td>\n",
       "      <td>-0.333433</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reference Random Forest</td>\n",
       "      <td>0.714153</td>\n",
       "      <td>0.672464</td>\n",
       "      <td>0.999912</td>\n",
       "      <td>0.365827</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Train only) Distilled Decision Tree Regressor</td>\n",
       "      <td>0.430430</td>\n",
       "      <td>0.411976</td>\n",
       "      <td>0.987538</td>\n",
       "      <td>-0.270110</td>\n",
       "      <td>6.045733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Train only) Distilled Random Forest Regressor</td>\n",
       "      <td>0.661445</td>\n",
       "      <td>0.629846</td>\n",
       "      <td>0.985770</td>\n",
       "      <td>0.456036</td>\n",
       "      <td>1.243802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Synthetic) Distilled Decision Tree Classifier</td>\n",
       "      <td>0.326341</td>\n",
       "      <td>0.320499</td>\n",
       "      <td>0.987538</td>\n",
       "      <td>-0.537350</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Synthetic) Distilled Random Forest Classifier</td>\n",
       "      <td>0.546734</td>\n",
       "      <td>0.537175</td>\n",
       "      <td>0.987538</td>\n",
       "      <td>0.240298</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Synthetic) Distilled Decision Tree Regressor</td>\n",
       "      <td>0.347716</td>\n",
       "      <td>0.341211</td>\n",
       "      <td>0.987538</td>\n",
       "      <td>-0.252018</td>\n",
       "      <td>4.957595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Synthetic) Distilled Random Forest Regressor</td>\n",
       "      <td>0.559612</td>\n",
       "      <td>0.541689</td>\n",
       "      <td>0.986389</td>\n",
       "      <td>0.357237</td>\n",
       "      <td>1.498531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Model  Test Accuracy  \\\n",
       "0                                  Reference LSTM       0.773898   \n",
       "1                                   Reference MLP       0.752257   \n",
       "2                         Reference Decision Tree       0.423526   \n",
       "3                         Reference Random Forest       0.714153   \n",
       "4  (Train only) Distilled Decision Tree Regressor       0.430430   \n",
       "5  (Train only) Distilled Random Forest Regressor       0.661445   \n",
       "6  (Synthetic) Distilled Decision Tree Classifier       0.326341   \n",
       "7  (Synthetic) Distilled Random Forest Classifier       0.546734   \n",
       "8   (Synthetic) Distilled Decision Tree Regressor       0.347716   \n",
       "9   (Synthetic) Distilled Random Forest Regressor       0.559612   \n",
       "\n",
       "   Reference Accuracy  Train Accuracy        R2  Test KL-Divergence  \n",
       "0            1.000000        0.987538  1.000000            0.000000  \n",
       "1            0.707780        0.997348  0.459579            3.902797  \n",
       "2            0.411710        0.999912 -0.333433                 inf  \n",
       "3            0.672464        0.999912  0.365827                 inf  \n",
       "4            0.411976        0.987538 -0.270110            6.045733  \n",
       "5            0.629846        0.985770  0.456036            1.243802  \n",
       "6            0.320499        0.987538 -0.537350                 inf  \n",
       "7            0.537175        0.987538  0.240298                 inf  \n",
       "8            0.341211        0.987538 -0.252018            4.957595  \n",
       "9            0.541689        0.986389  0.357237            1.498531  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dataframe_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-uniform",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-landing",
   "metadata": {},
   "source": [
    "This experiment, too, shows that Random Forests are superior to single Decision Trees, which is not surprising. Regressors are also better than classifiers in this case.\n",
    "\n",
    "Comparing the different metrics proves that accuracy is not a good suitable metric for measuring how similar the calculations are, because the non-distilled (\"reference\") models achieve strong accuracy results, while distilled models are visibly better when comparing the coefficient of determination (R2) and KL-Divergence.\n",
    "\n",
    "This comparison also shows that at least Regression Forests are to some extend able to simulate the calculations of neural networks. This becomes especially clear from looking at the KL-Divergence. However, this ability remains rather limited, which is visible from the overall performance and the exact scores.\n",
    "\n",
    "Two surprising findings are that the additional data doesn't increase or even decreases the performance of the distilled models, and that results for distilling the LSTM are very similar to results for distilling the MLP. Possible consequences are either that decision trees/random forests do not benefit very much from additional data, or that in this case, there is a domain mismatch between $\\mathcal{X}_{\\text{test}}$ and $\\mathcal{X}_{\\text{distill}}$. Another possible consequence is that the performance of distilled trees may be rather independent of the teacher model complexity.\n",
    "\n",
    "Summing up, these experiments have shown that distilling neural networks into decision trees/random forests yield only very limited success. The drop in performance is huge, and training an independent classifier on the original data always yields better results. Also bear in mind that trees cannot really process sequence data, which makes them an unpractical tool in NLP in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-stomach",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
